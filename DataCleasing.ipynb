{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from delta.tables import *\n",
    "from pyspark.sql.functions import *\n",
    "from datetime import datetime\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "import pyspark\n",
    "import torch\n",
    "\n",
    "# Set Spark Config\n",
    "\n",
    "def trim_whitespace(df: DataFrame) -> DataFrame:\n",
    "    for column in df.columns:\n",
    "        # Check if column type is String and trim if True\n",
    "        df = df.withColumn(column, trim(col(column))) if df.schema[column].dataType == \"StringType\" else df\n",
    "    return df\n",
    "    \n",
    "def load(url: str) -> DataFrame:\n",
    "    return trim_whitespace(spark.read.parquet(url))\n",
    "\n",
    "DT = load(\"YOUR_DATA_PATH\")\n",
    "\n",
    "# Configure Logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Model and Tokenizer Initialization (Singleton for PySpark workers)\n",
    "model_name = \"intfloat/multilingual-e5-large\"\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "class ModelWrapper:\n",
    "    \"\"\"Singleton class to load the model and tokenizer.\"\"\"\n",
    "    model = None\n",
    "    tokenizer = None\n",
    "\n",
    "    @staticmethod\n",
    "    def get_model_and_tokenizer():\n",
    "        if ModelWrapper.model is None or ModelWrapper.tokenizer is None:\n",
    "            logger.info(\"Loading model and tokenizer...\")\n",
    "            ModelWrapper.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "            ModelWrapper.model = AutoModel.from_pretrained(model_name).to(device).half()\n",
    "            logger.info(\"Model and tokenizer loaded successfully.\")\n",
    "        return ModelWrapper.tokenizer, ModelWrapper.model\n",
    "\n",
    "# UDF to Generate Embeddings with progress monitoring using logging\n",
    "@pandas_udf(\"array<float>\")\n",
    "def generate_embeddings_udf(full_name_series):\n",
    "    tokenizer, model = ModelWrapper.get_model_and_tokenizer()\n",
    "\n",
    "    embeddings_list = []\n",
    "    batch_size = 512  # Adjust for optimal GPU utilization\n",
    "    full_name_list = full_name_series.tolist()\n",
    "    total_batches = len(full_name_list) // batch_size + (1 if len(full_name_list) % batch_size > 0 else 0)\n",
    "\n",
    "    start_time = datetime.now()\n",
    "    logger.info(f\"Starting embedding generation for {len(full_name_list)} texts in {total_batches} batches.\")\n",
    "\n",
    "    for batch_idx in range(0, len(full_name_list), batch_size):\n",
    "        batch_texts = full_name_list[batch_idx:batch_idx + batch_size]\n",
    "        inputs = tokenizer(batch_texts, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "\n",
    "        # Perform inference and compute embeddings\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            embeddings = outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n",
    "\n",
    "        embeddings_list.extend(embeddings)\n",
    "\n",
    "        # Log progress\n",
    "        current_batch = batch_idx // batch_size + 1\n",
    "        elapsed_time = (datetime.now() - start_time).total_seconds()\n",
    "        estimated_time_remaining = (elapsed_time / current_batch) * (total_batches - current_batch)\n",
    "\n",
    "        logger.info(f\"Processed batch {current_batch}/{total_batches}. \"\n",
    "                    f\"Elapsed time: {elapsed_time:.2f}s, Estimated remaining: {estimated_time_remaining:.2f}s.\")\n",
    "\n",
    "    logger.info(\"Completed embedding generation.\")\n",
    "\n",
    "    # Convert the list of arrays to a Pandas Series for compatibility\n",
    "    return pd.Series([np.array(embedding, dtype=np.float32).tolist() for embedding in embeddings_list])\n",
    "\n",
    "# Add Embeddings Column with Monitoring\n",
    "logger.info(\"Adding embeddings column to the DataFrame...\")\n",
    "df = DT.withColumn(\"embedding\", generate_embeddings_udf(col(\"full_name\")))\n",
    "logger.info(\"Embedding generation completed. DataFrame updated.\")\n",
    "\n",
    "# Show the updated DataFrame\n",
    "df.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
