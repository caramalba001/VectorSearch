{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.cosmos import CosmosClient, PartitionKey\n",
    "import json\n",
    "import uuid\n",
    "\n",
    "# Initialize the Cosmos client\n",
    "cosmos_url = \"ํYOUR_COSMOS_URL\"\n",
    "cosmos_key = \"YOUR_COSMOS_KEY\"\n",
    "database_name = \"YOUR_DATABASE_NAME\"\n",
    "container_name = \"YOUR_CONTAINER_NAME\"\n",
    "\n",
    "client_cosmos = CosmosClient(cosmos_url, cosmos_key)\n",
    "\n",
    "# Create database if not exists\n",
    "database = client_cosmos.create_database_if_not_exists(id=database_name)\n",
    "\n",
    "# Create container without throughput configuration\n",
    "container = database.create_container_if_not_exists(\n",
    "    id=container_name,\n",
    "    partition_key=PartitionKey(path=\"/id\"),\n",
    ")\n",
    "\n",
    "print(\"Container created successfully in a serverless account.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_parquet(\"/Users/noppavitkanchitavorakul/Desktop/Azure-AI/Data\")\n",
    "\n",
    "df = df[['id_card', 'fname', 'lname', 'full_name']].rename(columns={'fname': 'first_name', 'lname': 'last_name'})\n",
    "\n",
    "chunk_size = 50000\n",
    "num_chunks = len(df) // chunk_size + (1 if len(df) % chunk_size != 0 else 0)\n",
    "df_chunks = [df.iloc[i*chunk_size:(i+1)*chunk_size] for i in range(num_chunks)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df_chunks[4].to_dict(orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# Initialize the Hugging Face model and tokenizer\n",
    "model_name = \"intfloat/multilingual-e5-large\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# Define batch size\n",
    "batch_size = 2000  # Adjust the batch size based on your requirements and API limits\n",
    "\n",
    "def get_embedding(text, tokenizer, model):\n",
    "    \"\"\"\n",
    "    Generate embedding for a single text using Hugging Face model.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text for which embedding is to be generated.\n",
    "        tokenizer (object): The tokenizer instance.\n",
    "        model (object): The model instance.\n",
    "\n",
    "    Returns:\n",
    "        list: The embedding for the input text.\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        embeddings = model(**inputs).last_hidden_state.mean(dim=1)\n",
    "    return embeddings.squeeze().tolist()\n",
    "\n",
    "def upload_batch_data(batch, container):\n",
    "    \"\"\"\n",
    "    Upload a batch of documents to Cosmos DB.\n",
    "\n",
    "    Args:\n",
    "        batch (list): List of records to upload.\n",
    "        container (object): Cosmos DB container object.\n",
    "    \"\"\"\n",
    "    documents = []\n",
    "    for record in batch:\n",
    "        document = {\n",
    "            \"id\": str(uuid.uuid4()),  # Generate a unique ID for each document\n",
    "            \"id_card\": record.get(\"id_card\"),\n",
    "            \"embedding\": record.get(\"embedding\"),\n",
    "            \"first_name\": record.get(\"first_name\"),\n",
    "            \"last_name\": record.get(\"last_name\"),\n",
    "            \"full_name\": record.get(\"full_name\")\n",
    "        }\n",
    "        documents.append(document)\n",
    "\n",
    "    # Upsert documents into the Cosmos DB container\n",
    "    for doc in documents:\n",
    "        container.upsert_item(doc)\n",
    "\n",
    "# Process data in batches\n",
    "for i in range(0, len(data), batch_size):\n",
    "    # Slice the data to create the current batch\n",
    "    batch = data[i:i + batch_size]\n",
    "\n",
    "    # Extract valid names from the batch\n",
    "    names = [record.get(\"full_name\", \"\").strip() for record in batch \n",
    "             if isinstance(record.get(\"full_name\", \"\"), str) and record.get(\"full_name\", \"\").strip()]\n",
    "\n",
    "    # Debugging and validation\n",
    "    print(f\"Processing batch {i // batch_size + 1}/{(len(data) + batch_size - 1) // batch_size}\")\n",
    "    print(f\"Number of valid names in batch: {len(names)}\")\n",
    "    \n",
    "    # Ensure all items are non-empty strings\n",
    "    assert all(isinstance(name, str) and name for name in names), \"Not all items in `names` are valid strings\"\n",
    "\n",
    "    # Generate embeddings using the `get_embedding` function\n",
    "    embeddings = [get_embedding(name, tokenizer, model) for name in names]\n",
    "        \n",
    "    # Attach embeddings back to the records\n",
    "    for record, embedding in zip(batch, embeddings):\n",
    "        record[\"embedding\"] = embedding\n",
    "\n",
    "    # Upload the batch data\n",
    "    upload_batch_data(batch, container)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the range of batches you want to process\n",
    "start_batch = 20\n",
    "end_batch = 25\n",
    "\n",
    "# Process only the specified batches\n",
    "for i in range((start_batch - 1) * batch_size, end_batch * batch_size, batch_size):\n",
    "    # Slice the data to create the current batch\n",
    "    batch = data[i:i + batch_size]\n",
    "\n",
    "    # Extract valid names from the batch\n",
    "    names = [record.get(\"full_name\", \"\").strip() for record in batch \n",
    "             if isinstance(record.get(\"full_name\", \"\"), str) and record.get(\"full_name\", \"\").strip()]\n",
    "\n",
    "    # Debugging and validation\n",
    "    print(f\"Processing batch {i // batch_size + 1}/{(len(data) + batch_size - 1) // batch_size}\")\n",
    "    print(f\"Number of valid names in batch: {len(names)}\")\n",
    "    \n",
    "    # Ensure all items are non-empty strings\n",
    "    assert all(isinstance(name, str) and name for name in names), \"Not all items in `names` are valid strings\"\n",
    "\n",
    "    # Generate embeddings using the `get_embedding` function\n",
    "    embeddings = [get_embedding(name, tokenizer, model) for name in names]\n",
    "        \n",
    "    # Attach embeddings back to the records\n",
    "    for record, embedding in zip(batch, embeddings):\n",
    "        record[\"embedding\"] = embedding\n",
    "\n",
    "    # Upload the batch data\n",
    "    upload_batch_data(batch, container)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_search(query, num_results=1):\n",
    "    query_embedding = get_embedding(query, tokenizer, model)\n",
    "    results = container.query_items(\n",
    "            query='SELECT TOP @num_results c.id, c.id_card, c.full_name, VectorDistance(c.embedding ,@embedding, true) AS SimilarityScore  FROM c ORDER BY VectorDistance(c.embedding,@embedding, true)',\n",
    "            parameters=[\n",
    "                {\"name\": \"@embedding\", \"value\": query_embedding}, \n",
    "                {\"name\": \"@num_results\", \"value\": num_results} \n",
    "            ],\n",
    "            enable_cross_partition_query=True)\n",
    "    \n",
    "    #correct this\n",
    "    return results\n",
    "\n",
    "query = \"สุชญา โตกุญาลัย\"\n",
    "results = vector_search(query)\n",
    "for result in results: \n",
    "    print(f\"Similarity Score: {result['SimilarityScore']}\")\n",
    "    print(f\"id_card: {result['id_card']}\")  \n",
    "    print(f\"full_name: {result['full_name']}\\n\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_list = []\n",
    "\n",
    "test1 = {\n",
    "    \"basic_wrong\": [\n",
    "        \"สุชยา โตคุณาลัย\",\n",
    "        \"สุชญา โตคุลาลัย\",\n",
    "        \"สุชญา โตกุณาลัย\",\n",
    "        \"สุชญา โตคุณาลัย์\",\n",
    "        \"สุชญา โตคุณะลัย\"\n",
    "    ],\n",
    "    \"medium_wrong\": [\n",
    "        \"สุชย่า โตคุนาลัย\",\n",
    "        \"สุชญา โตกุญาลัย\",\n",
    "        \"สุชณา โตคุลาลัย\",\n",
    "        \"สุชะยา โตคุณลัย\",\n",
    "        \"สุชณา โตคูณาลัย\"\n",
    "    ],\n",
    "    \"high_wrong\": [\n",
    "        \"สุชญา โตคุณลัยส์\",\n",
    "        \"สุชา โตคุนะไลน์\",\n",
    "        \"สุฌญา โตคูญาลัย\",\n",
    "        \"สุชา โตคุณไลน์\",\n",
    "        \"สุชนญา โตคุญลัย\"\n",
    "    ],\n",
    "    \"extreme_wrong\": [\n",
    "        \"สุชนญาร์ โตครุณละลัย\",\n",
    "        \"สุชาหญา โตคลุนาย\",\n",
    "        \"สุชะญา โตกุณาครัล\",\n",
    "        \"สุฌา โตคลุยาญ\",\n",
    "        \"สุชาหญ่า โตคนูยัลลาย\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "for category in ['basic_wrong', 'medium_wrong', 'high_wrong', 'extreme_wrong']:\n",
    "    for name in test1[category]:\n",
    "        # Perform the vector search and capture the results\n",
    "        results = vector_search(name)\n",
    "\n",
    "        if results:\n",
    "            for result in results:\n",
    "                # Add the case to the result dictionary\n",
    "                result['Case'] = category\n",
    "                result['Test_Data'] = name\n",
    "                # Append the result to the list\n",
    "                results_list.append(result)\n",
    "\n",
    "# # Convert the list of dictionaries to a DataFrame\n",
    "df1 = pd.DataFrame(results_list)\n",
    "\n",
    "## df2\n",
    "\n",
    "results_list = []\n",
    "\n",
    "test2 = {\n",
    "    \"basic_wrong\": [\n",
    "        \"สุชยา สุขปิติ\",\n",
    "        \"สุชญา สุคปิติ\",\n",
    "        \"สุชณา สุขปิติ\",\n",
    "        \"สุชญา สุกปิติ\",\n",
    "        \"สุชญ่า สุขปิติ\"\n",
    "    ],\n",
    "    \"medium_wrong\": [\n",
    "        \"สุชย่า สุขปิฏิ\",\n",
    "        \"สุชะญา สุคปิฏิ\",\n",
    "        \"สุชณา สุคปิตี\",\n",
    "        \"สุชญา สุกพิถิ\",\n",
    "        \"สุชณญา สุขปิติ\"\n",
    "    ],\n",
    "    \"high_wrong\": [\n",
    "        \"สุชา สุคปิถิ\",\n",
    "        \"สุฌญา สุคปิฐิ\",\n",
    "        \"สุชณญา สุกปิฏี\",\n",
    "        \"สุชาหญา สุกปิตี\",\n",
    "        \"สุชา สุคปิฏี\"\n",
    "    ],\n",
    "    \"extreme_wrong\": [\n",
    "        \"สุชนญาร์ สุกปิถี\",\n",
    "        \"สุชะหญ่า สุคปิฐะ\",\n",
    "        \"สุฌา สุกปิฏิฐะ\",\n",
    "        \"สุชหญ่า สุกปิฏิถี\",\n",
    "        \"สุชนญา สุคปิทิถะ\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "for category in ['basic_wrong', 'medium_wrong', 'high_wrong', 'extreme_wrong']:\n",
    "    for name in test2[category]:\n",
    "        # Perform the vector search and capture the results\n",
    "        results = vector_search(name)\n",
    "\n",
    "        if results:\n",
    "            for result in results:\n",
    "                # Add the case to the result dictionary\n",
    "                result['Case'] = category\n",
    "                result['Test_Data'] = name\n",
    "                # Append the result to the list\n",
    "                results_list.append(result)\n",
    "\n",
    "# # Convert the list of dictionaries to a DataFrame\n",
    "df2 = pd.DataFrame(results_list)\n",
    "\n",
    "## df3\n",
    "\n",
    "results_list = []\n",
    "\n",
    "test3 = {\n",
    "    \"basic_wrong\": [\n",
    "        \"สุกันญา โตไพร\",\n",
    "        \"สุกัญยา โตไพร\",\n",
    "        \"สุกัญญา โตไพ\",\n",
    "        \"สุกัญญา โตพร\",\n",
    "        \"สุกัญญา โตภาย\"\n",
    "    ],\n",
    "    \"medium_wrong\": [\n",
    "        \"สุกันยา โตพาย\",\n",
    "        \"สุกัณญา โตไพร์\",\n",
    "        \"สุกัญญ์ โตพรัย\",\n",
    "        \"สุกัญญา โทไพ\",\n",
    "        \"สุกัญญา โตภัย\"\n",
    "    ],\n",
    "    \"high_wrong\": [\n",
    "        \"สุคันญา โตพรย์\",\n",
    "        \"สุกัญย่า โทพราย\",\n",
    "        \"สุกันย่า โตพะไย\",\n",
    "        \"สุคัญญา โตพรัย\",\n",
    "        \"สุกัญญ์ โทไพรย\"\n",
    "    ],\n",
    "    \"extreme_wrong\": [\n",
    "        \"สุคัญญา โทพรัยย์\",\n",
    "        \"สุกันหญา โตพายญ์\",\n",
    "        \"สุขัญยา โตไพรยณ์\",\n",
    "        \"สุกัณหยา โตพรันย์\",\n",
    "        \"สุคัญย่า โทภายย์\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "for category in ['basic_wrong', 'medium_wrong', 'high_wrong', 'extreme_wrong']:\n",
    "    for name in test3[category]:\n",
    "        # Perform the vector search and capture the results\n",
    "        results = vector_search(name)\n",
    "\n",
    "        if results:\n",
    "            for result in results:\n",
    "                # Add the case to the result dictionary\n",
    "                result['Case'] = category\n",
    "                result['Test_Data'] = name\n",
    "                # Append the result to the list\n",
    "                results_list.append(result)\n",
    "\n",
    "# # Convert the list of dictionaries to a DataFrame\n",
    "df3 = pd.DataFrame(results_list)\n",
    "\n",
    "\n",
    "final_result = pd.concat([df1, df2, df3], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_result #50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_result #100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_result #150000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_result #200000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_result.to_excel('e5-200000.xlsx', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
